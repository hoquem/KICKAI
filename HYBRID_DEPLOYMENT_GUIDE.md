# üîÑ Hybrid Deployment Guide: Local Ollama + Railway Google AI

This guide explains how KICKAI uses a hybrid approach for optimal development and production deployment.

## üéØ Why Hybrid Approach?

### **Local Development (Ollama)**
- ‚úÖ **Free**: No API costs during development
- ‚úÖ **Fast**: No network latency
- ‚úÖ **Privacy**: All data stays local
- ‚úÖ **Offline**: Works without internet
- ‚úÖ **Custom Models**: Use any Ollama model

### **Railway Production (Google AI)**
- ‚úÖ **Reliability**: 99.9% uptime guarantee
- ‚úÖ **Scalability**: Handles multiple concurrent users
- ‚úÖ **Performance**: Optimized for production loads
- ‚úÖ **Monitoring**: Built-in analytics and logging
- ‚úÖ **Security**: Enterprise-grade security

## üîß Configuration System

The system automatically switches between providers based on environment:

### **Environment Detection**
```python
# Automatically detected from RAILWAY_ENVIRONMENT
environment = os.getenv('RAILWAY_ENVIRONMENT', 'development')
is_production = environment == 'production'
```

### **AI Provider Selection**
```python
if is_production:
    provider = 'google'  # Use Google AI
else:
    provider = 'ollama'  # Use Ollama
```

## üöÄ Local Development Setup

### **1. Install Ollama**
```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/download
```

### **2. Pull Model**
```bash
ollama pull llama3.1:8b-instruct-q4_0
```

### **3. Start Ollama**
```bash
ollama serve
```

### **4. Run KICKAI Locally**
```bash
# Set environment to development
export RAILWAY_ENVIRONMENT=development

# Run the bot
python run_telegram_bot.py
```

### **5. Test AI Commands**
```
"Add a new player named John Smith"
"List all players in the team"
"Schedule a match for Sunday"
```

## üöÄ Railway Production Setup

### **1. Environment Variables**
Set these in Railway:
```
RAILWAY_ENVIRONMENT=production
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your_supabase_anon_key
GOOGLE_API_KEY=your_google_ai_api_key
```

### **2. Deploy to Railway**
```bash
# Push to GitHub
git push origin main

# Railway automatically deploys
# Uses: python railway_main.py
```

### **3. Production Features**
- ü§ñ **Google AI**: Gemini Pro for natural language processing
- üóÑÔ∏è **Cloud Database**: Supabase for data persistence
- üìä **Monitoring**: Railway logs and health checks
- üîÑ **Auto-scaling**: Handles traffic spikes

## üìä Configuration Comparison

| Feature | Local (Ollama) | Railway (Google AI) |
|---------|----------------|---------------------|
| **Cost** | Free | Pay-per-use |
| **Speed** | Fast (local) | Fast (optimized) |
| **Reliability** | Good | Excellent |
| **Scalability** | Limited | Unlimited |
| **Privacy** | 100% local | Cloud-based |
| **Setup** | Manual | Automatic |

## üîÑ Switching Between Environments

### **Local Development**
```bash
# .env file
RAILWAY_ENVIRONMENT=development
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your_supabase_anon_key
# No GOOGLE_API_KEY needed
```

### **Railway Production**
```bash
# Railway environment variables
RAILWAY_ENVIRONMENT=production
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your_supabase_anon_key
GOOGLE_API_KEY=your_google_ai_api_key
```

## üß† AI Model Comparison

### **Ollama (Local)**
- **Model**: `llama3.1:8b-instruct-q4_0`
- **Size**: ~4GB
- **Performance**: Good for development
- **Cost**: Free
- **Setup**: Manual download

### **Google AI (Production)**
- **Model**: `gemini-pro`
- **Size**: Cloud-based
- **Performance**: Excellent for production
- **Cost**: ~$0.001 per 1K tokens
- **Setup**: API key only

## üõ†Ô∏è Development Workflow

### **1. Local Development**
```bash
# Start Ollama
ollama serve

# Run bot locally
python run_telegram_bot.py

# Test AI commands
# All responses come from local Ollama
```

### **2. Testing**
```bash
# Test CrewAI setup
python test_crewai_ollama_correct.py

# Test database connection
python test_database_setup.py

# Test bot functionality
python check_bot_status.py
```

### **3. Production Deployment**
```bash
# Commit changes
git add .
git commit -m "Update AI functionality"
git push origin main

# Railway automatically deploys
# Uses Google AI in production
```

## üìà Performance Monitoring

### **Local Development**
- Monitor Ollama logs: `ollama logs`
- Check model performance
- Test response quality

### **Railway Production**
- Railway dashboard: CPU, memory, logs
- Google AI usage: API dashboard
- Supabase analytics: Query performance

## üí∞ Cost Optimization

### **Development Costs**
- **Ollama**: Free (local)
- **Supabase**: Free tier (50MB)
- **Total**: $0/month

### **Production Costs**
- **Google AI**: ~$0.001 per 1K tokens
- **Railway**: ~$5/month (basic plan)
- **Supabase**: Free tier (50MB)
- **Total**: ~$5-10/month

## üîí Security Considerations

### **Local Development**
- ‚úÖ All data stays local
- ‚úÖ No API keys needed
- ‚úÖ Complete privacy

### **Railway Production**
- ‚úÖ Environment variables encrypted
- ‚úÖ HTTPS only
- ‚úÖ Railway security compliance
- ‚úÖ Google AI enterprise security

## üö® Troubleshooting

### **Local Issues**
```bash
# Check Ollama status
ollama list
ollama ps

# Restart Ollama
ollama stop
ollama serve

# Check model
ollama pull llama3.1:8b-instruct-q4_0
```

### **Railway Issues**
```bash
# Check logs
railway logs

# Check environment variables
railway variables

# Restart deployment
railway up
```

## üéØ Best Practices

### **Development**
1. Use Ollama for all local testing
2. Test with real Telegram groups
3. Monitor response quality
4. Keep Ollama updated

### **Production**
1. Monitor Google AI usage
2. Set up cost alerts
3. Use Railway health checks
4. Monitor Supabase performance

## üîÑ Migration Guide

### **From Local to Production**
1. Set up Railway project
2. Configure environment variables
3. Deploy code
4. Test with real users
5. Monitor performance

### **From Production to Local**
1. Install Ollama
2. Pull required models
3. Set development environment
4. Test locally
5. Debug issues

---

## üéâ Benefits of Hybrid Approach

1. **Cost Effective**: Free development, low-cost production
2. **Flexible**: Easy switching between environments
3. **Reliable**: Production-grade infrastructure
4. **Scalable**: Handles growth automatically
5. **Secure**: Enterprise-grade security in production

This hybrid approach gives you the best of both worlds: free, fast local development with Ollama, and reliable, scalable production deployment with Google AI on Railway!
